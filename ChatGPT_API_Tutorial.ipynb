{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d8695bb-2394-4d55-bd97-21594c82e0f4",
   "metadata": {},
   "source": [
    "In this Tutorial you will learn how to process data using Large Language Models in Python. We will focus on Open AI's Chat GPT and the possibility to use its API to formulate and run prompts in Python. In the folder, where this jupyter notebook is located, you will find all the necessary files to do this but this tutorial is going to explain the process in detail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710c3bea-d4e8-447f-a44b-c7f6181a19eb",
   "metadata": {},
   "source": [
    "Because using Chat GPT with API is not free, you need an OpenAI account that you pay for. Then you need to create your own API Key under https://platform.openai.com/api-keys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ff9aff-46a3-4bf9-b91e-0fb0ff6e85d5",
   "metadata": {},
   "source": [
    " In the folder where you are setting up your project create a file \".env\" and set the reference to your API Key like that: OPENAI_API_KEY=[YOUR API KEY GOES HERE]. If you like you can use the already existing .env file and simply put in your key. \n",
    "IMPORTANT: Be aware that each prompt that you will process using your API KEY is going to cost money depending on the specific model and prompt!\n",
    "An Overview of your cost is available on https://platform.openai.com/usage "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15cfff8-17c8-4ce7-ad0c-1415fa4de955",
   "metadata": {},
   "source": [
    "Once the .env variable is set, use \"pip install openai\" to install the OPEN AI module for python.\n",
    "Also use \"pip install dotenv\" to enable referencing your API Key in .env in your python script.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4ee4616-f9d1-4265-95d0-841085e4fe1c",
   "metadata": {},
   "source": [
    "Now create a new python file and import the following modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "020eea87-e15f-466d-a902-dae099e93f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "810a09b7-546b-4950-854c-8ec7393f0d81",
   "metadata": {},
   "source": [
    "Load your environment variables and initialize the client:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a28b3829-64ca-4648-9399-781f2b7d43b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPENAI_API_KEY loaded: True\n"
     ]
    }
   ],
   "source": [
    "# Load environment variables (expects OPENAI_API_KEY in .env)\n",
    "load_dotenv()\n",
    "api_key_present = os.getenv(\"OPENAI_API_KEY\") is not None\n",
    "print(f\"OPENAI_API_KEY loaded: {api_key_present}\")\n",
    "\n",
    "# Initialize the OpenAI client\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "529afd49-b25f-4566-96c5-e064a8aa9dba",
   "metadata": {},
   "source": [
    "Using the following code you define model, parameters and prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8039ef7f-1017-4412-bee1-c24237a14f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4o\", # current model\n",
    "    temperature=0, \n",
    "    # 0 means each output is as deterministic as possible\n",
    "    # note that some models like gpt-5-nano do not accept temperature setting at all\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"}, # system prompt can be left out altogether depending on your use case\n",
    "        {\"role\": \"user\", \"content\": \"Hello\"} # your prompt\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754ec3b-e38e-48c3-bb4b-906f03b63f37",
   "metadata": {},
   "source": [
    "In the code snippet above we are using gpt-4o as model and a temperature setting of 0 to make the output as deterministic as possible. Note that there are models like gpt-5-nano that do not accept the temperature parameter at all. \n",
    "Further we have defined a basic system prompt that defines the model's general behaviour. This system prompt can be left out entirely and is not mandatory.\n",
    "The user prompt defines our actual prompt that we need to make use of the current GPT model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c14da0-9b34-47bd-8751-d79806c7a564",
   "metadata": {},
   "source": [
    "Finally, we have to print the output: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24a1f8f-e38f-4008-828c-b8830396ca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382d8721-ae31-444a-b6f8-5413d9d76bca",
   "metadata": {},
   "source": [
    "Again: Be aware of the financial costs of using the API and make sure to check your current status on https://platform.openai.com/usage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d44454d-0359-47c4-87db-017b968b80fb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
